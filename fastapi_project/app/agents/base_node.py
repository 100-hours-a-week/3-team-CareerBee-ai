from abc import ABC, abstractmethod
import logging
from typing import Dict, Any, Optional
from app.agents.schema.resume_create_agent import ResumeAgentState


class BaseNode(ABC):
    """ëª¨ë“  ë…¸ë“œì˜ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self):
        """ê¸°ë³¸ ì´ˆê¸°í™” - ë¡œê±° ì„¤ì •"""
        self.logger = logging.getLogger(self.__class__.__name__)

    async def __call__(self, state: ResumeAgentState) -> ResumeAgentState:
        """
        LangGraph í˜¸í™˜ ì§„ì…ì 
        - ì—ëŸ¬ í•¸ë“¤ë§ê³¼ ë¡œê¹…ì„ í¬í•¨í•œ ë˜í¼ ë©”ì„œë“œ
        """
        node_name = self.__class__.__name__

        # ë°©ì–´ì  ë¡œê±° ì´ˆê¸°í™”
        if not hasattr(self, "logger"):
            self.logger = logging.getLogger(node_name)

        self.logger.info(f"ğŸš€ {node_name} ì‹¤í–‰ ì‹œì‘")
        self.logger.debug(
            f"ì…ë ¥ ìƒíƒœ: asked_count={state.asked_count}, info_ready={state.info_ready}"
        )

        try:
            # ìƒíƒœ ìœ íš¨ì„± ê²€ì¦
            self._validate_state(state)

            # ì‹¤ì œ ë…¸ë“œ ë¡œì§ ì‹¤í–‰
            result_state = await self.execute(state)

            # ê²°ê³¼ ê²€ì¦
            self._validate_result(result_state)

            self.logger.info(f"âœ… {node_name} ì‹¤í–‰ ì™„ë£Œ")
            self.logger.debug(
                f"ì¶œë ¥ ìƒíƒœ: asked_count={result_state.asked_count}, info_ready={result_state.info_ready}"
            )

            return result_state

        except Exception as e:
            self.logger.error(f"âŒ {node_name} ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            # ì—ëŸ¬ ë°œìƒì‹œ ì›ë³¸ ìƒíƒœì— ì—ëŸ¬ ì •ë³´ ì¶”ê°€
            return self._handle_error(state, e)

    @abstractmethod
    async def execute(self, state: ResumeAgentState) -> ResumeAgentState:
        """
        ì‹¤ì œ ë…¸ë“œ ë¡œì§ì„ êµ¬í˜„í•˜ëŠ” ë©”ì„œë“œ
        ê° ë…¸ë“œì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•¨
        """
        pass

    def _validate_state(self, state: ResumeAgentState) -> None:
        """
        ì…ë ¥ ìƒíƒœ ìœ íš¨ì„± ê²€ì¦
        í•„ìš”ì‹œ ê° ë…¸ë“œì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
        """
        if not isinstance(state, ResumeAgentState):
            raise TypeError(f"Expected ResumeAgentState, got {type(state)}")

        # ê¸°ë³¸ í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not hasattr(state, "inputs") or state.inputs is None:
            raise ValueError("State must have valid inputs")

    def _validate_result(self, state: ResumeAgentState) -> None:
        """
        ì¶œë ¥ ìƒíƒœ ìœ íš¨ì„± ê²€ì¦
        í•„ìš”ì‹œ ê° ë…¸ë“œì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
        """
        if not isinstance(state, ResumeAgentState):
            raise TypeError(f"Node must return ResumeAgentState, got {type(state)}")

    def _handle_error(
        self, state: ResumeAgentState, error: Exception
    ) -> ResumeAgentState:
        """
        ì—ëŸ¬ ì²˜ë¦¬ - ìƒíƒœì— ì—ëŸ¬ ì •ë³´ ì¶”ê°€
        """
        self.logger.error(f"ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(error)}")

        # ìƒíƒœë¥¼ ë³µì‚¬í•˜ì—¬ ì—ëŸ¬ ì •ë³´ ì¶”ê°€
        if hasattr(state, "model_copy"):
            error_state = state.model_copy()
        elif hasattr(state, "copy"):
            error_state = state.copy()
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ìƒíƒœ ì‚¬ìš©
            error_state = state

        # ì—ëŸ¬ ì •ë³´ë¥¼ stepì— ê¸°ë¡
        error_state.step = f"error_in_{self.__class__.__name__.lower()}"

        # CreateResumeNodeì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if self.__class__.__name__ == "CreateResumeNode":
            # ì—ëŸ¬ ë°œìƒí•´ë„ resume ë‚´ìš©ì´ ìˆë‹¤ë©´ ìœ ì§€
            if not error_state.resume and hasattr(self, "_last_generated_content"):
                error_state.resume = self._last_generated_content

            # ì—ëŸ¬ ì •ë³´ë¥¼ resumeì— ì¶”ê°€
            error_message = (
                f"\n\nâš ï¸ ì´ë ¥ì„œ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(error)}"
            )
            if error_state.resume:
                error_state.resume += error_message
            else:
                error_state.resume = f"ì´ë ¥ì„œ ìƒì„± ì™„ë£Œ{error_message}"

        return error_state

    def _log_state_change(
        self, before: ResumeAgentState, after: ResumeAgentState
    ) -> None:
        """
        ìƒíƒœ ë³€í™” ë¡œê¹… (ë””ë²„ê¹…ìš©)
        """
        changes = []

        if before.asked_count != after.asked_count:
            changes.append(f"asked_count: {before.asked_count} â†’ {after.asked_count}")

        if before.info_ready != after.info_ready:
            changes.append(f"info_ready: {before.info_ready} â†’ {after.info_ready}")

        if len(before.answers) != len(after.answers):
            changes.append(
                f"answers count: {len(before.answers)} â†’ {len(after.answers)}"
            )

        if before.pending_questions != after.pending_questions:
            changes.append(
                f"pending_questions: {len(before.pending_questions)} â†’ {len(after.pending_questions)}"
            )

        if changes:
            self.logger.debug(f"ìƒíƒœ ë³€í™”: {', '.join(changes)}")


class LLMBaseNode(BaseNode):
    """
    LLMì„ ì‚¬ìš©í•˜ëŠ” ë…¸ë“œë“¤ì˜ ê¸°ë³¸ í´ë˜ìŠ¤
    ê³µí†µ LLM ê´€ë ¨ ê¸°ëŠ¥ ì œê³µ
    """

    def __init__(self, llm=None):
        super().__init__()

        # LLM í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        if llm is None:
            from app.utils.llm_client import create_llm_client

            self.llm = create_llm_client(temperature=0.3)
        else:
            # ê¸°ì¡´ ChatOpenAI ê°ì²´ì¸ ê²½ìš° ë˜í•‘
            if hasattr(llm, "model_name") or hasattr(llm, "model"):
                from app.utils.llm_client import ChatLLM

                self.llm = ChatLLM(temperature=getattr(llm, "temperature", 0.3))
            else:
                self.llm = llm

    def _validate_state(self, state: ResumeAgentState) -> None:
        """LLM ë…¸ë“œ ì „ìš© ê²€ì¦"""
        super()._validate_state(state)

        if self.llm is None:
            raise ValueError(f"{self.__class__.__name__} requires LLM instance")

    async def _safe_llm_call(
        self,
        prompt: str,
        system_prompt: str = None,
        fallback_response: str = "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
    ) -> str:
        """
        ì•ˆì „í•œ LLM í˜¸ì¶œ - ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨
        """
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ íƒ€ì…ì— ë”°ë¥¸ í˜¸ì¶œ ë°©ì‹ ê²°ì •
            if hasattr(self.llm, "client") and hasattr(self.llm.client, "ainvoke"):
                # ìƒˆë¡œìš´ LLMClient ë˜ëŠ” ChatLLM ì‚¬ìš©
                response = await self.llm.ainvoke(prompt, system_prompt)
            elif hasattr(self.llm, "ainvoke"):
                # ì§ì ‘ LLMClient ì‚¬ìš©í•˜ê±°ë‚˜ ê¸°ì¡´ ChatOpenAI
                if system_prompt:
                    # system_prompt íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
                    import inspect

                    sig = inspect.signature(self.llm.ainvoke)
                    if len(sig.parameters) > 1:  # self ì œì™¸í•˜ê³  2ê°œ ì´ìƒ íŒŒë¼ë¯¸í„°
                        response = await self.llm.ainvoke(prompt, system_prompt)
                    else:
                        # system_promptë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ promptì— í•©ì¹¨
                        full_prompt = f"{system_prompt}\n\n{prompt}"
                        response = await self.llm.ainvoke(full_prompt)
                else:
                    response = await self.llm.ainvoke(prompt)
            else:
                self.logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM íƒ€ì…: {type(self.llm)}")
                return fallback_response

            # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
            if hasattr(response, "content"):
                return response.content.strip()
            else:
                return str(response).strip()

        except Exception as e:
            self.logger.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return fallback_response
