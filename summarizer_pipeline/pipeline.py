import json
from crawler_hankyung import crawl_hankyung
from embedder import add_news_to_chroma, delete_news_by_corp, setup_chroma_collection
from summarizer import generate_latest_issue

def generate_issue_summaries(corp_list):
    print("\nì „ì²´ ìš”ì•½ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    setup_chroma_collection()

    results = {}

    for corp in corp_list:
        try:
            print(f"\n[{corp}] ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘...")

            delete_news_by_corp(corp)
            articles = crawl_hankyung(corp, max_pages=3)
            for text, url in articles:
                if text and len(text) > 100:
                    add_news_to_chroma(text, corp=corp, url=url)

            summary = generate_latest_issue(corp)
            results[corp] = summary
            print(f"ìš”ì•½ ì™„ë£Œ: {corp}")

        except Exception as e:
            print(f"{corp} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ğŸ’¾ JSON ì €ì¥
    with open("data/latest_issues.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
        print("\nëª¨ë“  ìš”ì•½ JSON ì €ì¥ ì™„ë£Œ: data/latest_issues.json")