# crawler_hankyung.py

import requests
from bs4 import BeautifulSoup
import time

def crawl_hankyung(corp_name: str, max_pages: int = 3):
    """
    í•œê²½ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì—…ëª…ìœ¼ë¡œ ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§
    """
    base = "https://search.hankyung.com/search/news"
    params = {
        "query": corp_name,
        "period": "WEEK",
        "sort": "DATE/DESC,RANK/DESC",
    }

    seen = set()
    all_articles = []
    page = 1

    while True:
        print(f"ğŸŒ [í˜ì´ì§€ {page}] ìš”ì²­ ì¤‘...")
        params["page"] = page
        resp = requests.get(base, params=params, timeout=10)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        items = soup.select("div.txt_wrap > a")

        if not items:
            print("ğŸ“„ ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        for a in items:
            href = a.get("href", "")
            if href.startswith("https://www.hankyung.com") and href not in seen:
                seen.add(href)
                text = extract_article_text(href)
                if text:
                    all_articles.append((text, href))

        page += 1

        if max_pages and page > max_pages:
            break

        time.sleep(0.5)

    print(f"âœ… {len(all_articles)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ.")
    return all_articles

def extract_article_text(url: str):
    """
    ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
    """
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ê´‘ê³  ì œê±°
        for ad in soup.select("aside, iframe, div[class*=ad], div[id*=ad]"):
            ad.decompose()

        content = soup.select_one("div#articletxt") or soup.select_one("div.article-content")
        return content.get_text("\n", strip=True) if content else None

    except Exception as e:
        print(f"âŒ {url} ì½ê¸° ì‹¤íŒ¨: {e}")
        return None