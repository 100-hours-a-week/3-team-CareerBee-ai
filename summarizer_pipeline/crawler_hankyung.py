import requests
from bs4 import BeautifulSoup
import time

def crawl_hankyung(corp_name: str, top_k: int = 15, max_pages: int = 5):
    """
    í•œê²½ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì—…ëª…ìœ¼ë¡œ ì •í™•ë„ìˆœ ì •ë ¬ëœ ìƒìœ„ top_kê°œ ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘
    """
    base = "https://search.hankyung.com/search/news"
    seen = set()
    all_articles = []
    page = 1

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0.0.0 Safari/537.36"
        )
    }

    while len(all_articles) < top_k and page <= max_pages:
        print(f"ğŸŒ [í˜ì´ì§€ {page}] ìš”ì²­ ì¤‘...")

        params = {
            "query": corp_name,
            "period": "YEAR",
            "sort": "RANK/DESC,DATE/DESC",
            "page": page
        }

        try:
            resp = requests.get(base, params=params, headers=headers, timeout=10)
            resp.raise_for_status()
            print(f"ğŸ”— ìš”ì²­ URL: {resp.url}")
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

        soup = BeautifulSoup(resp.text, "html.parser")
        items = soup.select("div.txt_wrap > a")

        if not items:
            print("ğŸ“„ ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

        for a in items:
            href = a.get("href", "")
            if href.startswith("https://www.hankyung.com") and href not in seen:
                seen.add(href)
                text, date = extract_article_text(href, headers)
                if text:
                    all_articles.append((text, href, date))
                if len(all_articles) >= top_k:
                    break

        page += 1
        time.sleep(0.3)

    print(f"âœ… ì •í™•ë„ìˆœ ìƒìœ„ {len(all_articles)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ.")
    return all_articles


def extract_article_text(url: str, headers: dict):
    """
    í•œê²½ ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ë‚ ì§œ ì¶”ì¶œ
    """
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ë‚ ì§œ ì¶”ì¶œ
        date_tag = soup.select_one("span.txt-date")
        date_text = date_tag.get_text(strip=True) if date_tag else "unknown"

        # ê´‘ê³  ì œê±°
        for ad in soup.select("aside, iframe, div[class*=ad], div[id*=ad]"):
            ad.decompose()

        # ë³¸ë¬¸ ì¶”ì¶œ
        content = soup.select_one("div#articletxt") or soup.select_one("div.article-content")
        text = content.get_text("\n", strip=True) if content else None

        return (text, date_text)

    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì½ê¸° ì‹¤íŒ¨: {url} â†’ {e}")
        return (None, "unknown")